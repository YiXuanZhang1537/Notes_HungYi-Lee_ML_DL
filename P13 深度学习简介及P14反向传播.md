# P13 深度学习简介

## 深度学习的发展趋势

...

- 2009: GPU
- 2011: Start to be popular in speech recognition
- 2012: win ILSVRC image competition 感知机（Perceptron）非常像我们的逻辑回归（Logistics Regression）只不过是没有`sigmoid`激活函数。09年的GPU的发展是很关键的，使用GPU矩阵运算节省了很多的时间。

## 深度学习的三个步骤

![img](https://datawhalechina.github.io/leeml-notes/chapter13/res/chapter13-1.png)

### Step1：神经网络

![img](https://datawhalechina.github.io/leeml-notes/chapter13/res/chapter13-2.png)

**完全连接前馈神经网络**

![img](https://datawhalechina.github.io/leeml-notes/chapter13/res/chapter13-4.png)

**全连接和前馈的理解**

输入层：1层

隐藏层：N层

输出层：1层

![img](https://datawhalechina.github.io/leeml-notes/chapter13/res/chapter13-6.png)

**深度的理解**

Deep = Many hidden layers

到底可以有多少层呢？很难说

![img](https://datawhalechina.github.io/leeml-notes/chapter13/res/chapter13-7.png)

**矩阵运算**

![img](https://datawhalechina.github.io/leeml-notes/chapter13/res/chapter13-9.png)

**本质：通过隐藏层进行特征转换**

![img](https://datawhalechina.github.io/leeml-notes/chapter13/res/chapter13-12.png)

**实例：手写数字识别**

![img](https://datawhalechina.github.io/leeml-notes/chapter13/res/chapter13-13.png)

将图片中的像素点形成扁平数组，交给隐藏层进行处理，最后得到10维的数据，以最高得分进行判断

**FAQ**

- 多少层？ 每层有多少神经元？ 这个问我们需要用尝试加上直觉的方法来进行调试。对于有些机器学习相关的问题，我们一般用特征工程来提取特征，但是对于深度学习，我们只需要设计神经网络模型来进行就可以了。对于语音识别和影像识别，深度学习是个好的方法，因为特征工程提取特征并不容易。
- 结构可以自动确定吗？ 有很多设计方法可以让机器自动找到神经网络的结构的，比如进化人工神经网络（Evolutionary Artificial Neural Networks）但是这些方法并不是很普及 。
- 我们可以设计网络结构吗？ 可以的，比如 CNN卷积神经网络（Convolutional Neural Network ）

### Step2：模型评估

**损失实例**

![img](https://datawhalechina.github.io/leeml-notes/chapter13/res/chapter13-17.png)

总体损失：

![img](https://datawhalechina.github.io/leeml-notes/chapter13/res/chapter13-18.png)

### Step3：选择最优函数

（使用的方法是梯度下降）

![img](https://datawhalechina.github.io/leeml-notes/chapter13/res/chapter13-20.png)

这里也可以采用很多其他的工具包

# P14 反向传播

## 背景：

- 给到 \thetaθ (weight and bias)
- 先选择一个初始的 \theta^0θ0，计算 \theta^0θ0 的损失函数（Loss Function）设一个参数的偏微分
- 计算完这个向量（vector）偏微分，然后就可以去更新的你 \thetaθ
- 百万级别的参数（millions of parameters）
- 反向传播（Backpropagation）是一个比较有效率的算法，让你计算梯度（Gradient） 的向量（Vector）时，可以有效率的计算出来

## 链式法则

BP中最重要的部分就是链式法则

![img](https://datawhalechina.github.io/leeml-notes/chapter14/res/chapter14-2.png)

## 反向传播

1. 损失函数(Loss function)是定义在单个训练样本上的，也就是就算一个样本的误差，比如我们想要分类，就是预测的类别和实际类别的区别，是一个样本的，用L表示。
2. 代价函数(Cost function)是定义在整个训练集上面的，也就是所有样本的误差的总和的平均，也就是损失函数的总和的平均，有没有这个平均其实不会影响最后的参数的求解结果。
3. 总体损失函数(Total loss function)是定义在整个训练集上面的，也就是所有样本的误差的总和。也就是平时我们反向传播需要最小化的值。